{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper import Whisper, load_model, log_mel_spectrogram, pad_or_trim\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "from task1 import cer, wer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from torchaudio.compliance.kaldi import mfcc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn ( items : list ):\n",
    "    data = torch.stack([torch.Tensor(item) for item in items])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset ( LIBRISPEECH ):\n",
    "    def __init__(self, root, tokenizer, url=\"train-clean-100\"):\n",
    "        super().__init__(\".\", url=url, download=True)\n",
    "        self.root = root\n",
    "        self.tokenizer = tokenizer    \n",
    "    # def __make_frames(wav):\n",
    "    #     return mfcc(wav)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        wav, sr, text, speaker_id, chapter_id, utterance_id = super().__getitem__(index)\n",
    "        padded_wav = pad_or_trim(wav)\n",
    "        spectrogram = log_mel_spectrogram(padded_wav)\n",
    "\n",
    "        text = text.lower()\n",
    "\n",
    "        tokenized_text = self.tokenizer.sot_sequnce_including_notimestamps.extend(self.tokenizer.encode(text))\n",
    "        tokenized_labels = tokenized_text[1:] + [self.tokenizer.eot]\n",
    "\n",
    "        return spectrogram, tokenized_labels, tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer :\n",
    "    def __init__(self,\n",
    "            model: Whisper, train_dataset , valid_dataset, output_dir, lang, device, n_epoch, \n",
    "            batch_size=128, lr=1e-3):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.output_dir = output_dir\n",
    "        self.lang = lang\n",
    "        self.device = device\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "        self.valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "        self.optimizer = torch.optim.Adam(lr=lr)\n",
    "        self.criterion = wer\n",
    "\n",
    "\n",
    "    \n",
    "    def train_step ( self ):\n",
    "        epoch_loss = 0\n",
    "        self.model.train()    \n",
    "        \n",
    "        for X_train, y_train in tqdm(self.train_dataloader):  # The last batch can't be a src\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            X_train, y_train = X_train.to(self.device), y_train.to(self.device)\n",
    "            prediction = self.model.transcribe(X_train)               \n",
    "\n",
    "            # prediction = prediction.reshape(batch_size * seq_len, -1)   \n",
    "            loss = torch.tensor(self.criterion(prediction, y_train), device=self.device)\n",
    "            \n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip)\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item() * X_train.size()[0]\n",
    "        return epoch_loss / len(self.train_dataloader)\n",
    "    \n",
    "    def train ( self ):\n",
    "        for epoch in range(self.n_epoch):\n",
    "            print(f'STARTING EPOCH {epoch}')\n",
    "            train_loss = self.train_step()\n",
    "            val_loss, wer, cer = self.validate()\n",
    "            print(f'Training loss:   {train_loss}')\n",
    "            print(f'Validation loss: {val_loss}')\n",
    "            print(f'Validation WER:  {wer}')\n",
    "            print(f'Validation CER:  {cer}')\n",
    "\n",
    "    \n",
    "    def validate (self):\n",
    "        val_loss = 0\n",
    "        wers = 0\n",
    "        cers = 0\n",
    "        self.model.train()    \n",
    "        \n",
    "        for X_val, y_val in tqdm(self.valid_dataloader):  # The last batch can't be a src\n",
    "            with torch.no_grad():\n",
    "                X_val, y_val = X_val.to(self.device), y_val.to(self.device)\n",
    "                prediction = self.model.transcribe(X_val)               \n",
    "\n",
    "                # prediction = prediction.reshape(batch_size * seq_len, -1)   \n",
    "                loss = torch.tensor(self.criterion(prediction, y_val), device=self.device)\n",
    "                \n",
    "                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip)\n",
    "                val_loss += loss.item() * X_val.size()[0]\n",
    "                wers += wer(prediction, y_val)\n",
    "                cers += cer(prediction, y_val)\n",
    "        return val_loss / len(self.valid_dataloader), wers / len(self.valid_dataloader), cers / len(self.valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda if torch.cuda.is_available() else torch.cpu\n",
    "n_epoch = 1\n",
    "lang = 'en'\n",
    "tokenizer = get_tokenizer(True, language=lang, task='transcribe')\n",
    "train_dataset = MyDataset(\".\", tokenizer)\n",
    "valid_dataset = MyDataset(\".\", tokenizer, \"dev-clean\")\n",
    "output_dir = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m params \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mpatriotic_whisper_mixed_en_uk.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m\"\u001b[39m\u001b[39mtiny\u001b[39m\u001b[39m\"\u001b[39m , device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(params)\n",
      "File \u001b[1;32me:\\Courses\\Acoustic modelling\\day3\\venv\\lib\\site-packages\\torch\\serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[0;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Courses\\Acoustic modelling\\day3\\venv\\lib\\site-packages\\torch\\serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32me:\\Courses\\Acoustic modelling\\day3\\venv\\lib\\site-packages\\torch\\serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32me:\\Courses\\Acoustic modelling\\day3\\venv\\lib\\site-packages\\torch\\serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[0;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[1;32me:\\Courses\\Acoustic modelling\\day3\\venv\\lib\\site-packages\\torch\\serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32me:\\Courses\\Acoustic modelling\\day3\\venv\\lib\\site-packages\\torch\\serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32me:\\Courses\\Acoustic modelling\\day3\\venv\\lib\\site-packages\\torch\\serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "params = torch.load(\"patriotic_whisper_mixed_en_uk.pt\")\n",
    "model = load_model(\"tiny\" , device=device)\n",
    "model.load_state_dict(params)\n",
    "print(\"Patriotic model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer (\n",
    "    model ,\n",
    "    train_dataset ,\n",
    "    valid_dataset ,\n",
    "    output_dir ,\n",
    "    lang ,\n",
    "    device ,\n",
    "    n_epoch )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
